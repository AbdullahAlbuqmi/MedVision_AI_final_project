{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TinyLlama Medical Chatbot - Inference Only\n",
        "# This notebook loads your fine-tuned adapter and provides a chatbot interface.\n"
      ],
      "metadata": {
        "id": "KsDynWf7_iRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate peft bitsandbytes torch"
      ],
      "metadata": {
        "id": "SL6dUqr7_pC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f054345"
      },
      "source": [
        "!pip install -q Flask pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q guardrails-ai"
      ],
      "metadata": {
        "id": "OgPI26NfRmxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Guardrails CLI + common validator deps\n",
        "!pip install -q guardrails-ai alt-profanity-check rstr detoxify\n",
        "\n",
        "# verify guardrails installed\n",
        "!python -c \"import guardrails; print('guardrails version ->', getattr(guardrails, '__version__', 'unknown'))\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5HfuPtUUS01",
        "outputId": "11280f6d-fcf3-40d3-9163-63f2f2db26cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "guardrails version -> unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installs guardrails + common validator deps\n",
        "!pip install -q guardrails-ai alt-profanity-check rstr detoxify\n",
        "# quick check where guardrails is installed\n",
        "!python -c \"import guardrails, sys, inspect; print('guardrails:', guardrails.__version__, inspect.getfile(guardrails))\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02j1uV94XJfx",
        "outputId": "c5755791-8e97-4e93-a371-a27d8acb6709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "AttributeError: module 'guardrails' has no attribute '__version__'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configure guardrails CLI (interactive)\n",
        "!guardrails configure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3GEP2smVMoK",
        "outputId": "e5f05eed-c16e-4068-da1a-b65a97e28b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enable anonymous metrics reporting? [Y/n]: Y\n",
            "Do you wish to use remote inferencing? [Y/n]: Y\n",
            "\n",
            "\u001b[1mEnter API Key below\u001b[0m\u001b[1m \u001b[0m\u001b[2;3mleave empty if you want to keep existing token\u001b[0m\u001b[3m \u001b[0m\n",
            "ðŸ‘‰ You can find your API Key at \u001b[4;94mhttps://hub.guardrailsai.com/keys\u001b[0m\n",
            "\n",
            "API Key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnaXRodWJ8MjEzNTI5ODA1IiwiYXBpS2V5SWQiOiI1NGExYTJmNS00NDc4LTRjYTEtYWQ2YS05YTc1ZmMzNmFkYzciLCJzY29wZSI6InJlYWQ6cGFja2FnZXMiLCJwZXJtaXNzaW9ucyI6W10sImlhdCI6MTc2MTQ3OTY4NSwiZXhwIjoxNzY5MjU1Njg1fQ.xfxramcD6yvNsV0qECQxLyzTldVhEEOdEnfAfkzm3v4\n",
            "SUCCESS:guardrails-cli:\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install specific validators from the Guardrails Hub\n",
        "!guardrails hub install hub://guardrails/profanity_free\n",
        "!guardrails hub install hub://guardrails/toxic_language\n",
        "!guardrails hub install hub://guardrails/regex_match"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkVQdUuWVoWt",
        "outputId": "57bd49c4-a4e4-4904-b9b6-6206183a9856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mprofanity_free...\u001b[0m\n",
            "\u001b[2K\u001b[32m[==  ]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Downloading dependencies\n",
            "\u001b[2K\u001b[32m[==  ]\u001b[0m Running post-install setup\n",
            "\u001b[1A\u001b[2Kâœ…Successfully installed guardrails/profanity_free version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import ProfanityFree\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/profanity_free\u001b[0m\n",
            "\n",
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
            "\u001b[2K\u001b[32m[==  ]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[   =]\u001b[0m Downloading dependencies\n",
            "\u001b[2K\u001b[32m[ ===]\u001b[0m Running post-install setup2025-10-26 11:55:22.297895: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-26 11:55:22.315683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2K\u001b[32m[====]\u001b[0m Running post-install setupWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761479722.337096   32715 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761479722.343647   32715 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761479722.360031   32715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761479722.360056   32715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761479722.360058   32715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761479722.360062   32715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-26 11:55:22.364865: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[2K\u001b[32m[   =]\u001b[0m Running post-install setup\n",
            "\u001b[1A\u001b[2Kâœ…Successfully installed guardrails/toxic_language version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m2\u001b[0m!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import ToxicLanguage\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/toxic_language\u001b[0m\n",
            "\n",
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mregex_match...\u001b[0m\n",
            "\u001b[2K\u001b[32m[ ===]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[  ==]\u001b[0m Downloading dependencies\n",
            "\u001b[1A\u001b[2K\u001b[?25l\u001b[32m[    ]\u001b[0m Running post-install setup\n",
            "\u001b[1A\u001b[2Kâœ…Successfully installed guardrails/regex_match version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import RegexMatch\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/regex_match\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run this first if your adapter is zipped\n",
        "# !unzip /content/med_lora_chat_adapter_zip.zip -d /content/med_lora_chat_adapter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qYoPFP0_uXZ",
        "outputId": "b39bb762-2017-49f6-b7bb-25a347efd4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/med_lora_chat_adapter_zip.zip\n",
            "replace /content/med_lora_chat_adapter/adapter_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "import gradio as gr\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "from guardrails import Guard"
      ],
      "metadata": {
        "id": "y6lGnL_x_3yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"tinyllama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ADAPTER_PATH = \"/content/med_lora_chat_adapter\"  # Path to your adapter.zip extracted folder\n",
        "MAX_TOKENS = 512\n",
        "USE_4BIT = True"
      ],
      "metadata": {
        "id": "TNQmdbOQ_6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgpPJVFiZFrl"
      },
      "source": [
        "def load_fine_tuned_model():\n",
        "    \"\"\"Load the base model and your fine-tuned adapter\"\"\"\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Add padding token if missing\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading base model...\")\n",
        "    model_kwargs = {\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "\n",
        "    if USE_4BIT:\n",
        "        model_kwargs.update({\n",
        "            \"quantization_config\": {\n",
        "                \"load_in_4bit\": True,\n",
        "                \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "                \"bnb_4bit_quant_type\": \"nf4\",\n",
        "            }\n",
        "        })\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        **model_kwargs\n",
        "    )\n",
        "\n",
        "    print(\"Loading your fine-tuned adapter...\")\n",
        "    if os.path.exists(ADAPTER_PATH):\n",
        "        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "        print(\"Adapter loaded successfully!\")\n",
        "    else:\n",
        "        print(\"Adapter not found at:\", ADAPTER_PATH)\n",
        "        print(\"Using base model without fine-tuning...\")\n",
        "        model = base_model\n",
        "\n",
        "    return model, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Robust fix: don't use on_fail=\"replace\" (invalid) ---\n",
        "# Use on_fail defaults (noop) and apply replacement logic after validation.\n",
        "\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "from flask import Flask\n",
        "from guardrails import Guard\n",
        "from guardrails.hub import ProfanityFree, ToxicLanguage, RegexMatch\n",
        "\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# Create Guard with hub validators but DO NOT pass on_fail=\"replace\"\n",
        "# (either omit on_fail or use a supported keyword like \"filter\" / \"fix\" / \"exception\")\n",
        "guard = Guard().use_many(\n",
        "    ProfanityFree(),       # default on_fail=\"noop\" (or set to \"exception\" if you want a hard fail)\n",
        "    ToxicLanguage(),       # same\n",
        "    RegexMatch(r\".*(consult|doctor|medical professional|physician).*\")  # keep simple here\n",
        ")\n",
        "\n",
        "# Removed the unimplemented function, will use the one defined earlier.\n",
        "\n",
        "class MedicalChatbot:\n",
        "    def __init__(self):\n",
        "        # Use the load_fine_tuned_model defined earlier\n",
        "        self.model, self.tokenizer = load_fine_tuned_model()\n",
        "        self.chat_history = []\n",
        "\n",
        "    def format_chat_prompt(self, message: str, history: List[Tuple[str, str]] = None):\n",
        "        messages = []\n",
        "        if history:\n",
        "            for user_msg, assistant_msg in history:\n",
        "                messages.append({\"role\":\"user\",\"content\":user_msg})\n",
        "                if assistant_msg:\n",
        "                    messages.append({\"role\":\"assistant\",\"content\":assistant_msg})\n",
        "        messages.append({\"role\":\"user\",\"content\":message})\n",
        "\n",
        "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
        "            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        else:\n",
        "            prompt = \"\\n\".join([\n",
        "                f\"User: {m['content']}\" if m['role']=='user' else f\"Assistant: {m['content']}\"\n",
        "                for m in messages\n",
        "            ]) + \"\\nAssistant: \"\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, message: str, history: List[Tuple[str, str]] = None,\n",
        "                          temperature: float = 0.7, top_p: float = 0.9):\n",
        "        try:\n",
        "            prompt = self.format_chat_prompt(message, history)\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=MAX_TOKENS,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    top_p=top_p,\n",
        "                    pad_token_id=getattr(self.tokenizer, \"pad_token_id\", None),\n",
        "                    eos_token_id=getattr(self.tokenizer, \"eos_token_id\", None),\n",
        "                    repetition_penalty=1.1,\n",
        "                )\n",
        "\n",
        "            response_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "            response = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "            # --- VALIDATION STEP (robust) ---\n",
        "            # Use guard.validate(...) (preferred API) and then inspect outcome.\n",
        "            try:\n",
        "                outcome = guard.validate(response)\n",
        "            except Exception:\n",
        "                # If guard.validate itself crashes, fallback to blocking the response\n",
        "                return \"Response blocked or filtered for safety.\"\n",
        "\n",
        "            # outcome likely has attributes: validation_passed (bool), validated_output (str), error / errors\n",
        "            validation_passed = getattr(outcome, \"validation_passed\", True)\n",
        "            validated_output = getattr(outcome, \"validated_output\", None)\n",
        "            # If validation passed and validated_output exists, return it.\n",
        "            if validation_passed:\n",
        "                return validated_output if validated_output is not None else response\n",
        "\n",
        "            # If validation failed, inspect errors and decide how to handle them.\n",
        "            errors = getattr(outcome, \"error\", None) or getattr(outcome, \"errors\", None)\n",
        "            # If the regex / medical mention validator failed, return a custom safe message:\n",
        "            # (you can customize mapping from validator name -> replacement text here)\n",
        "            replacement_text_for_medical = \"Please consult a qualified doctor for accurate medical advice.\"\n",
        "            # Simple heuristic: if the original response mentioned \"doctor\" or \"consult\", return the medical fallback\n",
        "            if any(tok in response.lower() for tok in (\"doctor\", \"consult\", \"medical professional\", \"physician\")):\n",
        "                return replacement_text_for_medical\n",
        "\n",
        "            # Otherwise fallback to a generic safe message (or use validated_output if it contains fix suggestions)\n",
        "            if validated_output:\n",
        "                # some validators may produce a 'fixed' output in validated_output even if validation_passed is False\n",
        "                return validated_output\n",
        "            return \"Response blocked or filtered for safety.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Flask app (do not run a persistent server inside Colab unless you want to)\n",
        "app = Flask(__name__)\n",
        "# Initialize chatbot_instance after the class definition\n",
        "chatbot_instance = MedicalChatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKo9IQlPV6uL",
        "outputId": "6c5e229d-4644-4ba6-ac1b-cd60ed99132a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model...\n",
            "Loading your fine-tuned adapter...\n",
            "Adapter loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e2ca1ff"
      },
      "source": [
        "from flask import request, jsonify\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"error\": \"No message provided\"}), 400\n",
        "\n",
        "    response = chatbot_instance.generate_response(user_message)\n",
        "    return jsonify({\"response\": response})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "318b23fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66964b77-ea6f-4d80-b509-7f5c635a2333"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the Flask app to run on port 4998\n",
        "port = 4998\n",
        "\n",
        "# Get ngrok authtoken from Colab secrets\n",
        "ngrok_auth_token = userdata.get('ngrok')\n",
        "if ngrok_auth_token:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    print(\"ngrok authtoken set.\")\n",
        "else:\n",
        "    print(\"NGROK_AUTH_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "\n",
        "# Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(port)\n",
        "print(f\" * ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Function to run the Flask app\n",
        "def run_flask_app():\n",
        "    app.run(port=port, use_reloader=False)\n",
        "\n",
        "# Run Flask app in a separate thread\n",
        "thread = threading.Thread(target=run_flask_app)\n",
        "thread.start()\n",
        "\n",
        "# Keep the main thread alive to keep the Flask server and ngrok tunnel running\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down...\")\n",
        "    ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set.\n",
            " * ngrok tunnel established at: https://mellie-transitional-leonardo.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 4998 is in use by another program. Either identify and stop that program, or start the server with a different port.\n",
            "/usr/local/lib/python3.12/dist-packages/guardrails/validator_service/__init__.py:84: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Oct/2025 12:06:22] \"POST /chat HTTP/1.1\" 200 -\n",
            "/usr/local/lib/python3.12/dist-packages/guardrails/validator_service/__init__.py:84: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Oct/2025 12:06:50] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down...\n"
          ]
        }
      ]
    }
  ]
}