{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TinyLlama Medical Chatbot - Inference Only\n",
        "# This notebook loads your fine-tuned adapter and provides a chatbot interface.\n"
      ],
      "metadata": {
        "id": "KsDynWf7_iRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate peft bitsandbytes torch"
      ],
      "metadata": {
        "id": "SL6dUqr7_pC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34597af-7fe8-4401-b11d-6df504ee5614"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f054345"
      },
      "source": [
        "!pip install -q Flask pyngrok translate langdetect guardrails-ai alt-profanity-check rstr detoxify"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify guardrails installed\n",
        "!python -c \"import guardrails; print('guardrails version ->', getattr(guardrails, '__version__', 'unknown'))\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5HfuPtUUS01",
        "outputId": "8b857162-8341-48d5-d291-6c84227b771e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m758.6/758.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "guardrails version -> unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure guardrails CLI (interactive)\n",
        "!guardrails configure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3GEP2smVMoK",
        "outputId": "67050943-36a1-484a-9925-786ee6586482"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enable anonymous metrics reporting? [Y/n]: Y\n",
            "Do you wish to use remote inferencing? [Y/n]: Y\n",
            "\n",
            "\u001b[1mEnter API Key below\u001b[0m\u001b[1m \u001b[0mðŸ‘‰ You can find your API Key at \u001b[4;94mhttps://hub.guardrailsai.com/keys\u001b[0m\n",
            "\n",
            "API Key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnaXRodWJ8MjEzNTI5ODA1IiwiYXBpS2V5SWQiOiJhNDllNjE0Mi0xNjI2LTRiNDQtODdiZi03NjIwNjU5MjY4ZWYiLCJzY29wZSI6InJlYWQ6cGFja2FnZXMiLCJwZXJtaXNzaW9ucyI6W10sImlhdCI6MTc2MTUyNzI0MSwiZXhwIjoxNzY5MzAzMjQxfQ.TRB_KXHUDJ7wck2ApGfd41cf2e96i9TrRaM77aSgLNo\n",
            "\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific validators from the Guardrails Hub\n",
        "!guardrails hub install hub://guardrails/profanity_free\n",
        "!guardrails hub install hub://guardrails/toxic_language\n",
        "!guardrails hub install hub://guardrails/regex_match"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkVQdUuWVoWt",
        "outputId": "3ec6c099-d64c-4a23-c24d-84dd91f1ba3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mprofanity_free...\u001b[0m\n",
            "\u001b[2K\u001b[32m[=== ]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[=   ]\u001b[0m Downloading dependencies\n",
            "\u001b[1A\u001b[2KTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jsonschema/_format.py\", line 304, in <module>\n",
            "ModuleNotFoundError: No module named 'rfc3987'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/guardrails\", line 5, in <module>\n",
            "    from guardrails.cli import cli\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/__init__.py\", line 3, in <module>\n",
            "    from guardrails.guard import Guard\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/guard.py\", line 54, in <module>\n",
            "    from guardrails.run import Runner, StreamRunner\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/run/__init__.py\", line 1, in <module>\n",
            "    from guardrails.run.async_runner import AsyncRunner\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/run/async_runner.py\", line 13, in <module>\n",
            "    from guardrails.run.runner import Runner\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/run/runner.py\", line 22, in <module>\n",
            "    from guardrails.schema.validator import schema_validation\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/schema/validator.py\", line 3, in <module>\n",
            "    from jsonschema import Draft202012Validator, ValidationError\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jsonschema/__init__.py\", line 13, in <module>\n",
            "    from jsonschema._format import FormatChecker\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jsonschema/_format.py\", line 328, in <module>\n",
            "    from rfc3987_syntax import is_valid_syntax as _rfc3987_is_valid_syntax\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/__init__.py\", line 1, in <module>\n",
            "    from .syntax_helpers import *\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/syntax_helpers.py\", line 100, in <module>\n",
            "    is_valid_syntax_ipath_abempty = make_syntax_validator(\"ipath_abempty\")\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/syntax_helpers.py\", line 66, in make_syntax_validator\n",
            "    parser = Lark(grammar, start=rule_name, parser=RFC3987_SYNTAX_PARSER_TYPE)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/lark.py\", line 374, in __init__\n",
            "    self.grammar, used_files = load_grammar(grammar, self.source_path, self.options.import_paths, self.options.keep_all_tokens)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/load_grammar.py\", line 1416, in load_grammar\n",
            "    builder.load_grammar(grammar, source)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/load_grammar.py\", line 1241, in load_grammar\n",
            "    tree = _parse_grammar(grammar_text, grammar_name)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/load_grammar.py\", line 968, in _parse_grammar\n",
            "    tree = _get_parser().parse(text + '\\n', start)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/parser_frontends.py\", line 123, in parse\n",
            "    return self.parser.parse(stream, chosen_start, **kw)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/parsers/lalr_parser.py\", line 42, in parse\n",
            "    return self.parser.parse(lexer, start)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/parsers/lalr_parser.py\", line 88, in parse\n",
            "    return self.parse_from_state(parser_state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/parsers/lalr_parser.py\", line 100, in parse_from_state\n",
            "    for token in state.lexer.lex(state):\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/lexer.py\", line 534, in lex\n",
            "    yield self.next_token(state, parser_state)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/lexer.py\", line 612, in next_token\n",
            "    res = self.match(lex_state.text, line_ctr.char_pos)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/lexer.py\", line 607, in match\n",
            "    return self.scanner.match(text, pos)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lark/lexer.py\", line 387, in match\n",
            "    m = mre.match(text.text, pos, text.end)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/guardrails\", line 5, in <module>\n",
            "    from guardrails.cli import cli\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/__init__.py\", line 12, in <module>\n",
            "    from guardrails.hub.install import install\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails/hub/__init__.py\", line 5, in <module>\n",
            "    from guardrails_grhub_toxic_language import ToxicLanguage\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails_grhub_toxic_language/__init__.py\", line 1, in <module>\n",
            "    from .main import ToxicLanguage\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/guardrails_grhub_toxic_language/main.py\", line 5, in <module>\n",
            "    import detoxify\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/detoxify/__init__.py\", line 1, in <module>\n",
            "    from .detoxify import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/detoxify/detoxify.py\", line 1, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 416, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run this first if your adapter is zipped\n",
        "# !unzip /content/med_lora_chat_adapter_zip.zip -d /content/med_lora_chat_adapter/"
      ],
      "metadata": {
        "id": "6qYoPFP0_uXZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "import gradio as gr\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "from guardrails import Guard"
      ],
      "metadata": {
        "id": "y6lGnL_x_3yG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"tinyllama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ADAPTER_PATH = \"/content/med_lora_chat_adapter\"  # Path to your adapter.zip extracted folder\n",
        "MAX_TOKENS = 512\n",
        "USE_4BIT = True"
      ],
      "metadata": {
        "id": "TNQmdbOQ_6Id"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgpPJVFiZFrl"
      },
      "source": [
        "def load_fine_tuned_model():\n",
        "    \"\"\"Load the base model and your fine-tuned adapter\"\"\"\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Add padding token if missing\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading base model...\")\n",
        "    model_kwargs = {\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "\n",
        "    if USE_4BIT:\n",
        "        model_kwargs.update({\n",
        "            \"quantization_config\": {\n",
        "                \"load_in_4bit\": True,\n",
        "                \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "                \"bnb_4bit_quant_type\": \"nf4\",\n",
        "            }\n",
        "        })\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        **model_kwargs\n",
        "    )\n",
        "\n",
        "    print(\"Loading your fine-tuned adapter...\")\n",
        "    if os.path.exists(ADAPTER_PATH):\n",
        "        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "        print(\"Adapter loaded successfully!\")\n",
        "    else:\n",
        "        print(\"Adapter not found at:\", ADAPTER_PATH)\n",
        "        print(\"Using base model without fine-tuning...\")\n",
        "        model = base_model\n",
        "\n",
        "    return model, tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import List, Tuple\n",
        "from flask import Flask\n",
        "from guardrails import Guard\n",
        "from guardrails.hub import ProfanityFree, ToxicLanguage, RegexMatch\n",
        "from langdetect import detect\n",
        "from translate import Translator\n",
        "\n",
        "MAX_TOKENS = 512\n",
        "\n",
        "# Create Guard with hub validators\n",
        "guard = Guard().use_many(\n",
        "    ProfanityFree(),       # default on_fail=\"noop\" (or set to \"exception\" if you want a hard fail)\n",
        "    ToxicLanguage(),       # same\n",
        "    RegexMatch(r\".*(consult|doctor|medical professional|physician).*\")\n",
        ")\n",
        "\n",
        "class MedicalChatbot:\n",
        "    def __init__(self):\n",
        "        # Use the load_fine_tuned_model defined earlier\n",
        "        self.model, self.tokenizer = load_fine_tuned_model()\n",
        "        self.chat_history = []\n",
        "        # Initialize translator with a default to_lang, it will be updated in translate_text\n",
        "        self.translator = Translator(to_lang=\"en\")\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"Detect the language of a given text string.\"\"\"\n",
        "        try:\n",
        "            return detect(text)\n",
        "        except:\n",
        "            return 'en' # Default to English if detection fails\n",
        "\n",
        "    def translate_text(self, text: str, dest_lang: str, src_lang: str) -> str:\n",
        "        \"\"\"Translate a given text string from a source language to a target language.\"\"\"\n",
        "        try:\n",
        "            # Update the translator's destination language and source language\n",
        "            self.translator = Translator(to_lang=dest_lang, from_lang=src_lang)\n",
        "            translated_text = self.translator.translate(text)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text # Return original text on failure\n",
        "\n",
        "\n",
        "    def format_chat_prompt(self, message: str, history: List[Tuple[str, str]] = None):\n",
        "        messages = []\n",
        "        if history:\n",
        "            for user_msg, assistant_msg in history:\n",
        "                messages.append({\"role\":\"user\",\"content\":user_msg})\n",
        "                if assistant_msg:\n",
        "                    messages.append({\"role\":\"assistant\",\"content\":assistant_msg})\n",
        "        messages.append({\"role\":\"user\",\"content\":message})\n",
        "\n",
        "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
        "            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        else:\n",
        "            prompt = \"\\n\".join([\n",
        "                f\"User: {m['content']}\" if m['role']=='user' else f\"Assistant: {m['content']}\"\n",
        "                for m in messages\n",
        "            ]) + \"\\nAssistant: \"\n",
        "        return prompt\n",
        "\n",
        "\n",
        "    def generate_response(self, message: str, history: List[Tuple[str, str]] = None,\n",
        "                          temperature: float = 0.7, top_p: float = 0.9):\n",
        "        try:\n",
        "            # 1. Detect the language of the incoming message\n",
        "            original_lang = self.detect_language(message)\n",
        "            print(f\"Detected language: {original_lang}\")\n",
        "\n",
        "            llm_input_message = message\n",
        "            # 2. If the detected language is 'ar', translate to English\n",
        "            if original_lang == 'ar':\n",
        "                llm_input_message = self.translate_text(message, dest_lang='en', src_lang='ar')\n",
        "                print(f\"Translated Arabic to English for LLM: {llm_input_message}\")\n",
        "\n",
        "            # Use the (potentially translated) message for the LLM prompt\n",
        "            prompt = self.format_chat_prompt(llm_input_message, history)\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=MAX_TOKENS,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    top_p=top_p,\n",
        "                    pad_token_id=getattr(self.tokenizer, \"pad_token_id\", None),\n",
        "                    eos_token_id=getattr(self.tokenizer, \"eos_token_id\", None),\n",
        "                    repetition_penalty=1.1,\n",
        "                )\n",
        "\n",
        "            response_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "            response = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
        "            print(f\"LLM generated English response: {response}\")\n",
        "\n",
        "            # --- VALIDATION STEP (robust) ---\n",
        "            try:\n",
        "                outcome = guard.validate(response)\n",
        "            except Exception:\n",
        "                # If guard.validate itself crashes, fallback to blocking the response\n",
        "                final_response = \"Response blocked or filtered for safety.\"\n",
        "                print(f\"Validation failed, returning safety message: {final_response}\")\n",
        "                # Translate safety message back if original was Arabic\n",
        "                if original_lang == 'ar':\n",
        "                    final_response = self.translate_text(final_response, dest_lang='ar', src_lang='en')\n",
        "                    print(f\"Translated safety message back to Arabic: {final_response}\")\n",
        "                return final_response\n",
        "\n",
        "            validation_passed = getattr(outcome, \"validation_passed\", True)\n",
        "            validated_output = getattr(outcome, \"validated_output\", None)\n",
        "\n",
        "            final_response = response # Default to the LLM's raw response\n",
        "\n",
        "            if not validation_passed:\n",
        "                print(\"Validation failed.\")\n",
        "                replacement_text_for_medical = \"Please consult a qualified doctor for accurate medical advice.\"\n",
        "                # Simple heuristic: if the original response mentioned \"doctor\" or \"consult\", return the medical fallback\n",
        "                if any(tok in response.lower() for tok in (\"doctor\", \"consult\", \"medical professional\", \"physician\")):\n",
        "                    final_response = replacement_text_for_medical\n",
        "                    print(f\"Applying medical fallback due to validation failure: {final_response}\")\n",
        "                elif validated_output:\n",
        "                    # some validators may produce a 'fixed' output in validated_output even if validation_passed is False\n",
        "                    final_response = validated_output\n",
        "                    print(f\"Using validated output after failure: {final_response}\")\n",
        "                else:\n",
        "                    final_response = \"Response blocked or filtered for safety.\"\n",
        "                    print(f\"Applying generic safety message after validation failure: {final_response}\")\n",
        "            else:\n",
        "                 final_response = validated_output if validated_output is not None else response\n",
        "                 print(f\"Validation passed, using final response: {final_response}\")\n",
        "\n",
        "\n",
        "            # 5. If the original language was 'ar', translate the response back to Arabic\n",
        "            if original_lang == 'ar':\n",
        "                final_response = self.translate_text(final_response, dest_lang='ar', src_lang='en')\n",
        "                print(f\"Translated English response back to Arabic: {final_response}\")\n",
        "\n",
        "            # 6. Return the final response (translated if necessary)\n",
        "            return final_response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during response generation: {e}\")\n",
        "            # Translate error message back if original was Arabic\n",
        "            error_message = f\"Error generating response: {str(e)}\"\n",
        "            if original_lang == 'ar':\n",
        "                 error_message = self.translate_text(error_message, dest_lang='ar', src_lang='en')\n",
        "            return error_message\n",
        "\n",
        "# Re-initialize chatbot_instance after the class definition is updated\n",
        "chatbot_instance = MedicalChatbot()\n",
        "app = Flask(__name__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKo9IQlPV6uL",
        "outputId": "3169f457-c9e2-48aa-93f0-4fedaa2c10d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Loading base model...\n",
            "Loading your fine-tuned adapter...\n",
            "Adapter not found at: /content/med_lora_chat_adapter\n",
            "Using base model without fine-tuning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e2ca1ff"
      },
      "source": [
        "from flask import request, jsonify\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"error\": \"No message provided\"}), 400\n",
        "\n",
        "    # The generate_response method now handles translation internally\n",
        "    response = chatbot_instance.generate_response(user_message)\n",
        "    return jsonify({\"response\": response})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "318b23fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b9b347-b1d7-461f-c261-ec84ef8a5d30"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the Flask app to run on port 4998\n",
        "port = 4998\n",
        "\n",
        "# Get ngrok authtoken from Colab secrets\n",
        "ngrok_auth_token = userdata.get('ngrok')\n",
        "if ngrok_auth_token:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    print(\"ngrok authtoken set.\")\n",
        "else:\n",
        "    print(\"NGROK_AUTH_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "\n",
        "# Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(port)\n",
        "print(f\" * ngrok tunnel established at: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Function to run the Flask app\n",
        "def run_flask_app():\n",
        "    app.run(port=port, use_reloader=False)\n",
        "\n",
        "# Run Flask app in a separate thread\n",
        "thread = threading.Thread(target=run_flask_app)\n",
        "thread.start()\n",
        "\n",
        "# Keep the main thread alive to keep the Flask server and ngrok tunnel running\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down...\")\n",
        "    ngrok.kill()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set.\n",
            " * ngrok tunnel established at: https://mellie-transitional-leonardo.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:4998\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down...\n"
          ]
        }
      ]
    }
  ]
}