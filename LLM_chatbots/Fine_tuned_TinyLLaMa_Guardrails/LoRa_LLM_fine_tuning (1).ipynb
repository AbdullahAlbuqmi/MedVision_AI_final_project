{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers accelerate peft bitsandbytes"
      ],
      "metadata": {
        "id": "JDTkZfv7IU2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import textwrap\n",
        "import os"
      ],
      "metadata": {
        "id": "Twx_VkJ6v_ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "MODEL_NAME = \"tinyllama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ADAPTER_OUT = \"/content/med_lora_chat_adapter\"\n",
        "MAX_TOKENS = 512\n",
        "USE_4BIT = True\n",
        "NUM_TRAIN_ROWS = 5000\n",
        "VAL_SIZE = 500\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "OUTPUT_DIR = \"/content/tinyllama_med_lora_run\""
      ],
      "metadata": {
        "id": "PmuvGeTkJ5ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1) Load dataset & show head\n",
        "# -------------------------\n",
        "print(\"Loading dataset from Hugging Face...\")\n",
        "ds = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
        "\n",
        "print(\"\\nAvailable splits:\", ds.keys())\n",
        "print(\"\\nTrain split size (raw):\", len(ds[\"train\"]))\n",
        "\n",
        "# Select only the first NUM_TRAIN_ROWS rows from train\n",
        "train_raw = ds[\"train\"].select(range(min(NUM_TRAIN_ROWS, len(ds[\"train\"]))))\n",
        "print(f\"\\nSelected first {len(train_raw)} rows for experiments (will use {NUM_TRAIN_ROWS - VAL_SIZE} train + {VAL_SIZE} val).\")\n",
        "\n",
        "# Print column names and first 5 rows (head)\n",
        "print(\"\\nColumn names:\", train_raw.column_names)\n",
        "print(\"\\nFirst 5 rows (head):\")\n",
        "for i, ex in enumerate(train_raw.select(range(min(5, len(train_raw))))):\n",
        "    print(f\"\\n--- Row {i} ---\")\n",
        "    for k, v in ex.items():\n",
        "        print(f\"{k}: {str(v)[:400]}\")  # truncate long fields"
      ],
      "metadata": {
        "id": "LqfKJwFwwTHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2) Convert to chat-style messages\n",
        "# -------------------------\n",
        "def to_chat(example):\n",
        "    \"\"\"\n",
        "    Try to find user/patient text and assistant/doctor text from several possible column names.\n",
        "    Fallback: if there's a single 'text' or 'dialog', try to split on newline.\n",
        "    \"\"\"\n",
        "    # candidate keys (common variants)\n",
        "    user_keys = [\"Patient\", \"patient\", \"question\", \"Question\", \"prompt\", \"user\", \"User\", \"q\", \"Q\", \"text\"]\n",
        "    doc_keys  = [\"Doctor\", \"doctor\", \"answer\", \"Answer\", \"response\", \"Response\", \"reply\", \"Reply\", \"a\"]\n",
        "\n",
        "    user_text = None\n",
        "    doc_text = None\n",
        "\n",
        "    for k in user_keys:\n",
        "        if k in example and example[k]:\n",
        "            user_text = example[k]\n",
        "            break\n",
        "\n",
        "    for k in doc_keys:\n",
        "        if k in example and example[k]:\n",
        "            doc_text = example[k]\n",
        "            break\n",
        "\n",
        "    # Handle case where dataset has single \"text\" with lines\n",
        "    if (user_text is None or doc_text is None) and \"text\" in example and example[\"text\"]:\n",
        "        txt = example[\"text\"]\n",
        "        if isinstance(txt, str):\n",
        "            parts = [p.strip() for p in txt.split(\"\\n\") if p.strip()]\n",
        "            if len(parts) >= 2:\n",
        "                user_text = user_text or parts[0]\n",
        "                doc_text  = doc_text  or parts[1]\n",
        "    # Last fallback: put everything into user and empty assistant\n",
        "    user_text = (user_text or \"\").strip()\n",
        "    doc_text  = (doc_text  or \"\").strip()\n",
        "\n",
        "    # Ensure non-empty user (if empty, use a placeholder)\n",
        "    if user_text == \"\":\n",
        "        user_text = \"Hello, I have a medical question.\"\n",
        "\n",
        "    # If assistant reply missing, keep it short\n",
        "    if doc_text == \"\":\n",
        "        doc_text = \"I need more information to answer that.\"\n",
        "\n",
        "    return {\"messages\": [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": doc_text}]}\n",
        "\n",
        "print(\"\\nConverting selected rows to chat 'messages' pairs (this will be fast for 5k rows)...\")\n",
        "chat_small = train_raw.map(to_chat)\n",
        "\n",
        "# Quick sample check\n",
        "print(\"\\nSample converted message (index 0):\")\n",
        "print(chat_small[0][\"messages\"])"
      ],
      "metadata": {
        "id": "87XqigqLwfoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 3) Create train / val splits from the 5000 rows\n",
        "# -------------------------\n",
        "total_small = len(chat_small)\n",
        "assert total_small == NUM_TRAIN_ROWS or total_small < NUM_TRAIN_ROWS\n",
        "train_count = max(0, total_small - VAL_SIZE)\n",
        "train_ds = chat_small.select(range(train_count))\n",
        "val_ds   = chat_small.select(range(train_count, total_small))\n",
        "print(f\"\\nTrain rows: {len(train_ds)}, Val rows: {len(val_ds)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Tokenize using TinyLlama chat template\n",
        "# -------------------------\n",
        "print(\"\\nLoading tokenizer and preparing tokenization...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "# ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_chat(example):\n",
        "    # build the chat template string using TinyLlama helper\n",
        "    prompt = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
        "    tokens = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=MAX_TOKENS,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    # labels are a copy of input_ids (causal LM)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "print(\"Tokenizing train set (this may take a while)...\")\n",
        "train_tokenized = train_ds.map(tokenize_chat, remove_columns=train_ds.column_names)\n",
        "print(\"Tokenizing validation set...\")\n",
        "val_tokenized = val_ds.map(tokenize_chat, remove_columns=val_ds.column_names)\n",
        "\n",
        "# Set dataset format to torch tensors and keep only needed columns\n",
        "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tokenized.set_format(type=\"torch\", columns=cols)\n",
        "val_tokenized.set_format(type=\"torch\", columns=cols)\n",
        "print(\"\\nTokenization complete. Example shapes (train index 0):\")\n",
        "print({k: train_tokenized[0][k].shape for k in cols})"
      ],
      "metadata": {
        "id": "k_LEHYzmwooC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5) Load base model and attach LoRA adapter\n",
        "# -------------------------\n",
        "print(\"\\nLoading base model and preparing LoRA...\")\n",
        "\n",
        "model_kwargs = {\"device_map\": \"auto\"}\n",
        "if USE_4BIT:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "print(\"\\nTrainable parameters (should be LoRA params only):\")\n",
        "try:\n",
        "    model.print_trainable_parameters()\n",
        "except Exception as e:\n",
        "    # fallback listing if the wrapper doesn't provide the helper\n",
        "    for n, p in model.named_parameters():\n",
        "        if p.requires_grad:\n",
        "            print(n, p.shape)"
      ],
      "metadata": {
        "id": "94emRt7hwya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 6) TrainingArguments + Trainer\n",
        "# -------------------------\n",
        "print(\"\\nPreparing Trainer...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "h5S2l267xC3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 7) Launch training\n",
        "# -------------------------\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(\"\\nTraining finished. Metrics:\")\n",
        "print(train_result)\n",
        "\n",
        "# -------------------------\n",
        "# 8) Save adapter & tokenizer\n",
        "# -------------------------\n",
        "print(f\"\\nSaving adapter + tokenizer to {ADAPTER_OUT} ...\")\n",
        "os.makedirs(ADAPTER_OUT, exist_ok=True)\n",
        "model.save_pretrained(ADAPTER_OUT)\n",
        "tokenizer.save_pretrained(ADAPTER_OUT)\n",
        "print(\"Saved.\")"
      ],
      "metadata": {
        "id": "Ome0-tWwxMz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 9) Interactive chat loop\n",
        "# -------------------------\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "print(\"\\nStarting interactive chat with the fine-tuned adapter. Type 'exit' to quit.\\n\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_4bit=USE_4BIT,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_OUT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# helper to keep a bounded conversation history (prevent runaway token length)\n",
        "def recent_messages(conv, max_user_assistant_pairs=6):\n",
        "    # keep system + last N user/assistant pairs\n",
        "    if len(conv) <= 1:\n",
        "        return conv\n",
        "    system = conv[0]\n",
        "    pairs = conv[1:]\n",
        "    # keep last 2*max_user_assistant_pairs items from pairs\n",
        "    keep = pairs[-(2 * max_user_assistant_pairs):] if len(pairs) > (2 * max_user_assistant_pairs) else pairs\n",
        "    return [system] + keep\n",
        "\n",
        "# initialize conversation\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"\"\"You are Dr. AI, a professional, empathetic, and friendly medical assistant. Follow these rules:\n",
        "\n",
        "1. Provide clear, concise, and easy-to-understand answers in a conversational style.\n",
        "2. Offer practical and safe guidance whenever possible.\n",
        "3. If unsure about a condition or if the situation could be serious, advise the user to consult a qualified healthcare professional.\n",
        "4. Ask clarifying questions if needed and remember recent conversation context.\n",
        "5. Be polite, empathetic, and avoid unnecessary medical jargon (or explain it simply).\n",
        "6. always answer in simplest terms.\n",
        "\n",
        "Always respond responsibly, keeping the user's safety and understanding in mind.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "device = next(model.parameters()).device  # model device (works with device_map=\"auto\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
        "            print(\"Session ended.\")\n",
        "            break\n",
        "\n",
        "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # build a truncated recent conversation to avoid exceeding tokenizer max length\n",
        "        conv_for_prompt = recent_messages(conversation, max_user_assistant_pairs=6)\n",
        "        prompt = tokenizer.apply_chat_template(conv_for_prompt, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "        # Trim the prompt from the decoded text to get only the assistant reply\n",
        "        reply = full_output[len(prompt):].strip()\n",
        "        if reply == \"\":\n",
        "            reply = \"I'm sorry — I couldn't generate a response. Could you rephrase?\"\n",
        "\n",
        "        print(\"\\nDoctor:\\n\")\n",
        "        print(textwrap.fill(reply, width=100))\n",
        "        print()\n",
        "\n",
        "        # append assistant response to conversation memory\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nInterrupted by user — session ended.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ct2SkudPx57F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Compress the folder into a zip file\n",
        "shutil.make_archive(\"/content/med_lora_chat_adapter_zip\", 'zip', \"/content/med_lora_chat_adapter\")\n",
        "files.download(\"/content/med_lora_chat_adapter_zip.zip\")"
      ],
      "metadata": {
        "id": "11a4KBztGNqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}